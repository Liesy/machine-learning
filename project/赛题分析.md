#### 心跳信号分类预测

#### 山东大学 计算机科学与技术学院 19级智能班 李阳

---

## 问题分析

#### 多分类问题	0，1，2，3四类

#### 数据量大	10w训练集样本，2w测试集样本

#### 特征是一串心跳序列

- 实际是多个特征伪装成的一个特征
- 无匿名特征（与贷款违约预测相比），无缺省值
- 特征之间关联性较大，主要是时序上的关联

#### 结果提交的是4种不同心跳信号预测的概率，而非单一的预测所属分类

## 数据处理

#### 通过替换数据类型、用category类代替object类的方法来减小内存占用

##### 从int8 -> int64占用内存逐渐增大，float16 -> float64同理。预处理数据考察DataFrame中的每一列的最大值与最小值，然后为每一列分配最合适的数据类型，可以大大减少占用的内存。

#### 将**heartbeat_signals**列分割成小的特征

##### 处理前

![原数据](D:\大二下\机器学习\machine-learning\project\原数据.jpg)

##### 处理后

![处理后数据](D:\大二下\机器学习\machine-learning\project\处理后数据.jpg)

#### 观察到**id**列无缺省值无异常值，所以在模型拟合时可以去掉，直接用DataFrame的默认索引代替id值

#### 可视化

![label图](D:\大二下\机器学习\machine-learning\project\label图.png)



## 方法选用

### AdaBoost + 决策树

1. 最常见的AdaBoost元分类器是决策树，sklearn中默认使用一层的决策树来作为它的元分类器
2. 尝试以不同层数的决策树作为元分类器，精确度先增加后减少

![adaboost1](D:\大二下\机器学习\machine-learning\project\adaboost1.jpg)

### AdaBoost + SVM

尝试在一个分类问题中找到一个较优的元分类器

SVM支持向量机，一般用于二分类模型，支持线性可分和非线性划分。SVM中用到的核函数有线性核'linear'、多项式核函数pkf以及高斯核函数rbf。当训练数据线性可分时，一般用线性核函数，直接实现可分；当训练数据不可分时，需要使用核技巧，将训练数据映射到另一个高维空间，使再高维空间中，数据可线性划分，但需要注意的是，若样本n和特征m很大时，且特征m>>n时，需要用线性核函数，因为此时考虑高斯核函数的映射后空间维数更高，更复杂，也容易过拟合，此时使用高斯核函数的弊大于利，选择使用线性核会更好；若样本n一般大小，特征m较小，此时进行高斯核函数映射后，不仅能够实现将原训练数据再高维空间中实现线性划分，而且计算方面不会有很大的消耗，因此利大于弊，适合用高斯核函数；若样本n很大，但特征m较小，同样难以避免计算复杂的问题，因此会更多考虑线性核。

2000条数据跑了43分钟才出结果，实际效果和一层决策树的AdaBoost一致，并不是很优

![adaboost2](D:\大二下\机器学习\machine-learning\project\adaboost2.jpg)

对比之下，还是决策树更适合与AdaBoost搭配

### 随机森林 Random Forest

1. **随机森林适用于拥有大型数据集的情况**，可以处理数千个输入变量而无需变量删除
2. 不需要很多参数调整就可以达到不错的效果
3. 有一种估算缺失数据的有效方法，并在大部分数据丢失时保持准确性
4. 可以保存生成的林以备将来用于其他数据
5. 随着森林建设的进展，它会产生对**泛化误差**的内部无偏估计

![randomforest](D:\大二下\机器学习\machine-learning\project\randomforest.jpg)

### 优化手段

#### 可以看出以8层决策树为元分类器的AdaBoost和随机森林的准确率较高，表现较好

#### 最终选择随机森林算法

1.  PCA 会进行降维操作，这可以减少随机森林要处理的特征数量，因此 PCA 可能有助于加快随机森林模型的训练速度。计算成本高是随机森林的最大缺点之一，如果只想简单地拥有最佳性能的模型，并且可以牺牲解释特征的重要性，那么 PCA 可能会很有用，但这并不是必须的
2.  以SVM为元分类器的AdaBoost算法准确率不高的原因可能是在创建元分类器时没有对特征进行标准化处理，应用SVM需要进行标准化，但树形结构不用
3.  后续对模型进行超参数调优，进一步提高分类准确率

#### 参数调优

1. 参数n_estimators对性能的影响最大，所以要使该参数对模型在未知数据上的评估性能的影响幅度最小
2. max_depth次之，最大深度也代表了最高复杂度
3. min_samples_leaf和min_samples_split并列
4. max_features对模型的影响较小
5. criterion一般使用gini，具体需要视情况而定

##### 模型太复杂或者太简单，都会让泛化误差高，我们追求的是位于中间的平衡点

##### max_depth，min_samples_leaf和min_samples_split是剪枝参数，是减小复杂度的参数

![1](D:\大二下\机器学习\machine-learning\project\slides\image\1.jpg)

![rfc_after_adjust](D:\大二下\机器学习\machine-learning\project\slides\image\rfc_after_adjust.jpg)

## 名词解释

#### **泛化误差**

##### 用来衡量一个学习机器推广未知数据的能力，即根据从样本数据中学习到的规则能够应用到新数据的能力

## 参考

1. [随机森林 - Random Forest](https://zhuanlan.zhihu.com/p/44695084)

2. [泛化误差和经验误差](https://blog.csdn.net/seasongirl/article/details/80889488)

3. [心跳信号分类预测_Task 2 天池技术讨论区](https://tianchi.aliyun.com/forum/postDetail?spm=5176.12586969.1002.36.3cf245515sJeOB&postId=195918)

4. [决策树VS随机森林](https://www.jiqizhixin.com/articles/2020-06-11-6)

5. [决策树与随机森林之间的关系](https://blog.csdn.net/qq_39777550/article/details/107312048)

6. [利用AdaBoost元算法提高分类性能](https://blog.csdn.net/baidu_31657889/article/details/93891552?utm_source=app&app_version=4.7.1)

7. [使用Adaboost进行分类性能提升](https://blog.csdn.net/weixin_41677876/article/details/106154591?utm_source=app&app_version=4.7.1)

8. [各种机器学习算法的应用场景](https://www.zhihu.com/question/26726794/answer/151282052)
